mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/gpt-4o.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/gpt4o_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/gpt4o_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/gpt4o_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/gpt-4o-mini.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/gpt4o-mini_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/gpt4o-mini_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/gpt4o-mini_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/gpt-4.1-mini.json  --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/gpt4.1-mini_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/gpt4.1-mini_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/gpt4.1-mini_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/gpt-4.1-nano.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/gpt4.1-nano_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/gpt4.1-nano_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/gpt4.1-nano_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/o3-mini.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/o3-mini_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/o3-mini_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/o3-mini_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/o3.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/o3_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/o3_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/o3_evaluation_tasks_verified_report.md

mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/o4-mini.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/o4-mini_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/healthcare/results/o4-mini_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/o4-mini_evaluation_tasks_verified_report.md 

nohup mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/xlam_2_32b.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/xlam_2_32b_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30 > benchmarks/healthcare/logs/xlam_2_32b_mix_task_evaluation.log 2>&1 &
nohup mcp-eval analyze --predictions benchmarks/healthcare/results/xlam_2_32b_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/xlam_2_32b_evaluation_tasks_verified_report.md > benchmarks/healthcare/logs/xlam_2_32b_evaluation_tasks_verified_report.log 2>&1 &

nohup mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/xlam_2_70b.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/xlam_2_70b_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30 > benchmarks/healthcare/logs/xlam_2_70b_mix_task_evaluation.log 2>&1 &
nohup mcp-eval analyze --predictions benchmarks/healthcare/results/xlam_2_70b_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/xlam_2_70b_evaluation_tasks_verified_report.md > benchmarks/healthcare/logs/xlam_2_70b_evaluation_tasks_verified_report.log 2>&1 &

nohup mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/qwen3_30b_a3b.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/qwen3_30b_a3b_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30 > benchmarks/healthcare/logs/qwen3_30b_a3b_mix_task_evaluation.log 2>&1 &
nohup mcp-eval analyze --predictions benchmarks/healthcare/results/qwen3_30b_a3b_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/qwen3_30b_a3b_evaluation_tasks_verified_report.md > benchmarks/healthcare/logs/qwen3_30b_a3b_evaluation_tasks_verified_report.log 2>&1 &

nohup mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/qwen3_32b.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/qwen3_32b_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30 > benchmarks/healthcare/logs/qwen3_32b_mix_task_evaluation.log 2>&1 &
nohup mcp-eval analyze --predictions benchmarks/healthcare/results/qwen3_32b_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/qwen3_32b_evaluation_tasks_verified_report.md > benchmarks/healthcare/logs/qwen3_32b_evaluation_tasks_verified_report.log 2>&1 &

nohup mcp-eval evaluate --server mcp_servers/healthcare/server.py --model-config benchmarks/healthcare/eval_models/mistral_small_24b.json --tasks-file data/healthcare/evaluation_tasks_verified.jsonl --output benchmarks/healthcare/results/mistral_small_24b_mix_task_evaluation.json --prompt-file benchmarks/healthcare/evaluation_prompt.json --max-turns 30 > benchmarks/healthcare/logs/mistral_small_24b_mix_task_evaluation.log 2>&1 &
nohup mcp-eval analyze --predictions benchmarks/healthcare/results/mistral_small_24b_mix_task_evaluation.json --ground-truth data/healthcare/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/healthcare/report/mistral_small_24b_evaluation_tasks_verified_report.md > benchmarks/healthcare/logs/mistral_small_24b_evaluation_tasks_verified_report.log 2>&1 &
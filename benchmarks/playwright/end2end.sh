mcp-eval generate-tasks --server @playwright/mcp --model gpt-4.1-2025-04-14 --num-tasks 600 --prompt-file benchmarks/playwright/data_generation/task_generation_prompt.json --output data/playwright/evaluation_tasks.jsonl
mcp-eval verify-tasks --server @playwright/mcp --tasks-file data/playwright/evaluation_tasks.jsonl --model gpt-4.1-2025-04-14 --output data/playwright/evaluation_tasks_verified.jsonl --prompt-file benchmarks/playwright/data_generation/task_verification_prompt.json --num-tasks 500
mcp-eval evaluate --server @playwright/mcp --model-config benchmarks/playwright/eval_models/gpt-4o.json --tasks-file data/playwright/evaluation_tasks_verified.jsonl --output benchmarks/playwright/results/gpt4o_mix_task_evaluation.json --prompt-file benchmarks/playwright/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/playwright/results/gpt4o_mix_task_evaluation.json --ground-truth data/playwright/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/playwright/report/gpt4o_evaluation_tasks_verified_report.md
# evaluate gpt-4o-mini
mcp-eval evaluate --server @playwright/mcp --model-config benchmarks/playwright/eval_models/gpt-4o-mini.json --tasks-file data/playwright/evaluation_tasks_verified.jsonl --output benchmarks/playwright/results/gpt4o-mini_mix_task_evaluation.json --prompt-file benchmarks/playwright/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/playwright/results/gpt4o-mini_mix_task_evaluation.json --ground-truth data/playwright/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/playwright/report/gpt4o-mini_evaluation_tasks_verified_report.md
# evaluate gpt-4.1-mini
mcp-eval evaluate --server @playwright/mcp --model-config benchmarks/playwright/eval_models/gpt-4.1-mini.json  --tasks-file data/playwright/evaluation_tasks_verified.jsonl --output benchmarks/playwright/results/gpt-4.1-mini_mix_task_evaluation.json --prompt-file benchmarks/playwright/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/playwright/results/gpt-4.1-mini_mix_task_evaluation.json --ground-truth data/playwright/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/playwright/report/gpt-4.1-mini_evaluation_tasks_verified_report.md
# evaluate gpt-4.1-nano
mcp-eval evaluate --server @playwright/mcp --model-config benchmarks/playwright/eval_models/gpt-4.1-nano.json --tasks-file data/playwright/evaluation_tasks_verified.jsonl --output benchmarks/playwright/results/gpt-4.1-nano_mix_task_evaluation.json --prompt-file benchmarks/playwright/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/playwright/results/gpt-4.1-nano_mix_task_evaluation.json --ground-truth data/playwright/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/playwright/report/gpt-4.1-nano_evaluation_tasks_verified_report.md
# evaluate o3-mini
mcp-eval evaluate --server @playwright/mcp --model-config benchmarks/playwright/eval_models/o3-mini.json --tasks-file data/playwright/evaluation_tasks_verified.jsonl --output benchmarks/playwright/results/o3-mini_mix_task_evaluation.json --prompt-file benchmarks/playwright/evaluation_prompt.json --max-turns 30
mcp-eval analyze --predictions benchmarks/playwright/results/o3-mini_mix_task_evaluation.json --ground-truth data/playwright/evaluation_tasks_verified.jsonl --generate-report --report-model gpt-4.1-2025-04-14 --report-output benchmarks/playwright/report/o3-mini_evaluation_tasks_verified_report.md 
# LLM Judger Analysis

This module provides comprehensive analysis capabilities for LLM judger results, specifically designed to analyze trajectory and completion score files generated by the LLM judger.

## Features

### 1. **Statistical Analysis**
- Summary statistics for trajectory and completion scores
- Distribution analysis (high/medium/low score counts)
- Combined score calculations and analysis
- Aspect-level analysis for both trajectory and completion evaluations

### 2. **Correlation Analysis**
- Correlation between trajectory and completion scores
- Analysis of original success vs. LLM judge scores
- Cross-aspect correlation insights

### 3. **Pattern Identification**
- High performers (‚â•0.9 combined score)
- Low performers (‚â§0.5 combined score)
- Tasks with large gaps between trajectory and completion scores
- Consistent performers (scores within 0.1 of each other)
- Aspect-specific weaknesses identification

### 4. **Insights Generation**
- Automated insight generation based on analysis results
- Performance gap identification
- Success alignment analysis
- Aspect-specific recommendations

### 5. **AI Report Generation**
- Generate comprehensive AI-powered performance reports using OpenAI models
- Professional markdown reports with structured tables and analysis
- Executive summaries and actionable recommendations
- Support for generating reports from existing analysis files

## Usage

### Command Line Interface

```bash
# Basic analysis
python -m mcp_eval_llm.cli.main judge-rubric \
    --trajectory-file path/to/trajectory_scores.json \
    --completion-file path/to/completion_scores.json

# With output file and verbose mode
python -m mcp_eval_llm.cli.main judge-rubric \
    --trajectory-file path/to/trajectory_scores.json \
    --completion-file path/to/completion_scores.json \
    --output path/to/analysis_results.json \
    --verbose
```

### Arguments

- `--trajectory-file`: Path to trajectory scores JSON file (required)
- `--completion-file`: Path to completion scores JSON file (required)
- `--output-dir`, `-o`: Directory to save analysis results and AI report (default: same as input files)
- `--verbose`, `-v`: Print detailed analysis information (optional)
- `--generate-report`: Generate AI-powered performance report: 1=enabled (default), 0=disabled (analysis only)
- `--report-model`: Model to use for AI report generation (default: gpt-4o)

### Example with Real Files

```bash
# Default: Generate both analysis and AI report (saves to same directory as input files)
python -m mcp_eval_llm.cli.main judge-rubric \
    --trajectory-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_trajectory_scores.json \
    --completion-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_completion_scores.json

# Specify custom output directory
python -m mcp_eval_llm.cli.main judge-rubric \
    --trajectory-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_trajectory_scores.json \
    --completion-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_completion_scores.json \
    --output-dir custom/analysis/directory

# Analysis only (disable AI report generation)
python -m mcp_eval_llm.cli.main judge-rubric \
    --trajectory-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_trajectory_scores.json \
    --completion-file benchmarks/yfinance/results/gpt4o_mix_task_evaluation_judged_completion_scores.json \
    --generate-report 0
```

## Input File Format

### Trajectory Scores File
```json
[
  {
    "task_id": "unique-task-id",
    "task_name": "Task Name",
    "original_success": true,
    "trajectory_score": 0.95,
    "trajectory_scores": {
      "planning": 1.0,
      "execution_flow": 0.9,
      "tool_selection": 1.0,
      "tool_usage": 0.9,
      "adaptability": 1.0,
      "efficiency": 0.9,
      "context_awareness": 0.95
    },
    "trajectory_comments": "Detailed evaluation comments..."
  }
]
```

### Completion Scores File
```json
[
  {
    "task_id": "unique-task-id",
    "task_name": "Task Name",
    "original_success": true,
    "completion_score": 0.92,
    "completion_scores": {
      "requirement_coverage": 1.0,
      "accuracy": 0.9,
      "completeness": 0.9,
      "usefulness": 0.85
    },
    "completion_comments": "Detailed evaluation comments..."
  }
]
```

## Output Format

The analysis results are saved in JSON format with the following structure:

```json
{
  "summary_statistics": {
    "total_tasks": 185,
    "trajectory_stats": {
      "count": 185,
      "mean": 0.970,
      "median": 1.0,
      "std_dev": 0.080,
      "min": 0.143,
      "max": 1.0,
      "high_scores": 178,
      "medium_scores": 6,
      "low_scores": 1
    },
    "completion_stats": { /* similar structure */ },
    "combined_stats": { /* similar structure */ },
    "task_coverage": {
      "tasks_with_both_scores": 185,
      "tasks_with_trajectory_only": 0,
      "tasks_with_completion_only": 0
    },
    "aspect_analysis": {
      "trajectory": {
        "planning": { "mean": 0.969, "median": 1.0, /* ... */ },
        /* other aspects */
      },
      "completion": {
        "requirement_coverage": { "mean": 0.966, "median": 1.0, /* ... */ },
        /* other aspects */
      }
    }
  },
  "correlations": {
    "trajectory_completion_correlation": 0.725,
    "original_success_correlation": {
      "success_trajectory_mean": 0.974,
      "failure_trajectory_mean": 0.143,
      "success_completion_mean": 0.928,
      "failure_completion_mean": 0.0,
      "success_count": 184,
      "failure_count": 1
    }
  },
  "patterns": {
    "high_performers": [
      {
        "task_id": "task-id",
        "task_name": "Task Name",
        "trajectory_score": 1.0,
        "completion_score": 1.0,
        "combined_score": 1.0
      }
    ],
    "low_performers": [ /* similar structure */ ],
    "trajectory_completion_gaps": [ /* tasks with large score gaps */ ],
    "consistent_performers": [ /* tasks with consistent scores */ ],
    "aspect_weaknesses": {
      "trajectory": { /* aspects with low scores */ },
      "completion": { /* aspects with low scores */ }
    }
  },
  "insights": [
    "Overall Performance: Average combined score of 0.946 with 180 high-performing tasks (‚â•0.8)",
    "Strong Correlation: Trajectory and completion scores are highly correlated (r=0.725)",
    /* more insights */
  ],
  "metadata": {
    "analysis_type": "llm_judger_results",
    "version": "1.0"
  }
}
```

## AI Report Output

When using `--generate-report`, the tool creates a comprehensive markdown report with:

### Executive Summary
- Overall performance overview
- Key performance indicators

### Structured Tables
- Overall Performance Summary (trajectory vs completion vs combined scores)
- Aspect Performance Breakdown with improvement recommendations
- Performance Distribution by score ranges
- Top and Bottom Performers identification
- Correlation Analysis Results

### Analysis Sections
- **Strengths and Weaknesses**: Key areas of excellence and improvement
- **Areas for Improvement**: Specific aspects requiring attention
- **Actionable Recommendations**: Concrete steps for performance enhancement
- **Conclusion**: Summary and next steps

### Example AI Report Structure
```markdown
# LLM Judger Performance Report

## Executive Summary
The LLM judger evaluation indicates a robust overall performance...

## Overall Performance Summary
| Metric | Trajectory Score | Completion Score | Combined Score |
|--------|------------------|------------------|----------------|
| Mean   | 0.970           | 0.923           | 0.946         |

## Aspect Performance Breakdown
| Aspect | Mean Score | Performance Level | Improvement Needed |
|--------|------------|-------------------|-------------------|
| Planning | 0.969 | Excellent | Minimal |

## Actionable Recommendations
1. **Enhance Accuracy**: Focus on improving the accuracy of completion tasks...
2. **Optimize Efficiency**: Streamline processes and decision-making pathways...
```

## Analysis Output

The tool provides a comprehensive console output with:

### üìä Overview
- Total tasks analyzed
- Task coverage (both scores, trajectory only, completion only)

### üõ§Ô∏è Trajectory Scores & ‚úÖ Completion Scores
- Mean ¬± standard deviation
- Range (min - max)
- Score distribution (high/medium/low)

### üéØ Combined Scores
- Statistics for the average of trajectory and completion scores

### üîó Correlations
- Trajectory ‚Üî Completion correlation coefficient
- Original success impact analysis

### üé≠ Aspect Analysis
- Detailed breakdown by evaluation aspects
- Color-coded display (green: ‚â•0.8, yellow: 0.6-0.8, red: <0.6)

### üîç Key Patterns
- Count of high/low/consistent performers
- Score gap analysis

### üí° Key Insights
- Automated insights and recommendations
- Performance analysis
- Areas for improvement

## Integration

### Programmatic Usage

```python
from mcp_eval_llm.cli.llm_judger.analyze import analyze_llm_judger_results

# Run analysis
analyze_llm_judger_results(
    trajectory_file="path/to/trajectory_scores.json",
    completion_file="path/to/completion_scores.json",
    output_file="path/to/analysis_results.json",
    verbose=True
)
```

### Loading Results

```python
import json

with open("analysis_results.json", "r") as f:
    results = json.load(f)

# Access statistics
stats = results["summary_statistics"]
print(f"Average trajectory score: {stats['trajectory_stats']['mean']:.3f}")

# Access insights
for insight in results["insights"]:
    print(f"‚Ä¢ {insight}")
```

## Related Commands

- `llm-judge`: Generate LLM judge scores for evaluation results
- `analyze`: Analyze static tool evaluation results
- `evaluate`: Run model evaluation on tasks

## Examples

See `examples/llm_judger_analysis_example.py` for a complete usage example. 